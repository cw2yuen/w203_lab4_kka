---
title: 'Lab 4: Reducing Crime'
author: "Kim Vignola, Kiersten Hendersen, Aaron Yuen"
date: "August 17, 2017"
geometry: margin=1.5cm
output:
  pdf_document: default
  html_document: default
---

#Section 1: Introduction
The authors, Kim, Kiersten and Aaron, were hired to provide research for a political campaign in North Carolina to understand the determinants of crime (both correlational and causal) using exploratory data analysis and OLS regression. The end goal is to leverage the data to provide policy suggestions that are applicable to local government to reduce crime.

The provided cross-sectional dataset consists of statistics for a selection of counties for a given year. Data for 90 counties and 25 variables for each county were provided. 

For this analysis, the following assumptions were made:

* The 90 counties provided were randomly sampled among the 100 counties in North Carolina.


#Section 2: Exploratory Analysis

## Data Load and Library Imports

Reading the data and loading the right libraries:
```{r warning=FALSE, message=FALSE}
library(car)
library(corrplot)
library(lmtest)
library(sandwich)
library(stargazer)

data = read.csv("crime_v2.csv")
```

## Univariate Variable Analysis

There are 90 data points and 25 variables
```{r}
nrow(data)
colnames(data)
```

There doesn't seem to be any NAs in the dataset.
```{r}
apply(!is.na(data[,]), MARGIN = 2, mean)
```

The following summarizes the different variables types based on the variable desciptions and basic understanding of the data:

1. Rates, averages, and probabilities - crime, probarr, probconv, probsen, avgsen, police, density, pctmin, mix, ymale
2. $ variables - tax, wagecon, wagetuc, wagetrd, wagefir, wageser, wagemfg, wagefed, wagesta, wageloc
3. Indicator variables - west, central, urban. No base categories are in the dataset (e.g. non-west/central, rural)
4. Other miscellaneous variables - X, county, year

The remaining EDA focuses the analysis on only the key variables of interest.

### Crimes Committed per Person (crime)

Crime is the main dependent variable of interest. Looking at the histogram of crime, the distribution tends to be right skewed. Taking the log of crime tends to make the histgram appear more normal. As a result, for our modeling, we will proceed with using log of crime as the dependent variable.
```{r fig.height=3}
par(mfrow=c(1,2))
hist(data$crime, breaks=20,
     main="Hist of Crime",
     xlab="Crimes committed per person", cex=0.7)
hist(log(data$crime), breaks=20,
     main="Hist of Log of Crime",
     xlab="Log of crimes committed per person", cex=0.7)
```

### Probability of Conviction (probconv)

Probconv seems to have values greater than 1.0, which is unexpected given that probability values are supposed to be between 0.0 to 1.0. The variable description does not seem to indicate how the variable is calculated. After discussing this in the lectures and office hours, we will assume that probconv values above 1.0 is okay, and that the higher the value, the higher the probabilty of conviction.

The histogram of probconv does not seem to look very normal. That said, taking the log of a probabilty does not seem to make sense from an interpretability perspective. For example, an "increase is 10% of probability" is not intuitively interpretable. As a result, for probconv no log transformation will be applied. It also does not look like other transforms would be suitable for this variable.
```{r fig.height=3}
summary(data$probconv)
hist(data$probconv, breaks=50,
     main="Histogram of Probability of Conviction",
     xlab="Probability of conviction")
```

### Probability of Prison Sentence (probsen)

Similarly, probsen seems to have values greater than 1.0, which is unexpected given that probability values are supposed to be between 0.0 to 1.0. After discussing this in the lectures and office hours, we will assume that probsen values above 1.0 is okay, and that the higher the value, the higher the probabilty of prison sentence.

Also, similar to probconv, for probsen no log transformation will be applied. It also does not look like other transforms would be suitable for this variable.
```{r fig.height=3}
summary(data$probsen)
hist(data$probsen, breaks=50,
     main="Histogram of Probability of Prison Sentence",
     xlab="Probability of prison sentence")
```

### Police per Capita (police)

Looking at the histogram of police, the distribution tends to be right skewed. Taking the log of police tends to make the histgram appear more normal. As a result, for our modeling, we will proceed with using log of police.
```{r fig.height=3}
summary(data$police)
par(mfrow=c(1,2))
hist(data$police, breaks=20,
     main="Hist of Police/Capita",
     xlab="Police per capita", cex.axis = 0.7)
hist(log(data$police), breaks=20,
     main="Hist of Log of Police/Capita",
     xlab="Log of Police per capita", cex.axis = 0.7)
```


### People per Sq. Mile (density)

Looking at the histogram of density, the distribution tends to be right skewed. Taking the log of density tends to make the histgram appear more normal. As a result, for our modeling, we will proceed with using log of density.
```{r fig.height=3}
summary(data$density)
par(mfrow=c(1,2))
hist(data$density, breaks=20,
     main="Hist of People/Sq. Mile",
     xlab="Probability of people per sq. mile")
hist(log(data$density), breaks=20,
     main="Hist of Log of people/Sq. Mile",
     xlab="Log of people per sq. mile")
```

### Percentage of Minority, 1980 (pctmin)

The data and histogram for pctmin looks expected. However, the data is between 0 - 100 whereas other percentage variables is between 0.0 and 1.0; We may want to transform this variable to keep things consistent from an interpretability perspective. This will be taken care of in the data transformation section.
```{r fig.height=3}
summary(data$pctmin)
hist(data$pctmin, breaks=50,
     main="Histogram of Percentage of Minority",
     xlab="Probability of percentage of minority")
```
\pagebreak

## Wage variables (wage*)

Looking at the histogram of the wage variables, there is no obvious case for supporting any transforms on the variables, since for most cases the distribution does not look very skewed.
```{r}
par(mfrow=c(3,3))
hist(data$wagecon, breaks=20, main="Hist of wagecon", xlab="wagecon", ylab="Frequency")
hist(data$wagetuc, breaks=20, main="Hist of wagetuc", xlab="wagetuc", ylab="")
hist(data$wagetrd, breaks=20, main="Hist of wagetrd", xlab="wagetrd", ylab="")
hist(data$wagefir, breaks=20, main="Hist of wagefir", xlab="wagefir", ylab="Frequency")
hist(data$wageser, breaks=20, main="Hist of wageser", xlab="wageser", ylab="")
hist(data$wagemfg, breaks=20, main="Hist of wagemfg", xlab="wagemfg", ylab="")
hist(data$wagefed, breaks=20, main="Hist of wagefed", xlab="wagefed", ylab="Frequency")
hist(data$wagesta, breaks=20, main="Hist of wagesta", xlab="wagesta", ylab="")
hist(data$wageloc, breaks=20, main="Hist of wageloc", xlab="wageloc", ylab="")
```

Focusing on wageser, there seems to be one data point that looks to be an extreme outlier. Looking further, it seems to be coming from data point 84. This data point has an extremely high value for probsen and wageser. For our model, we will remove this datapoint, as we have found that this data point tend to have high Cook's distance if we were to include it in our models.
```{r}
data[data$X == 84,c("probsen", "wageser")]
```

\pagebreak

## Transformed and Filtered Dataset
Based on the univariate analysis, the following transformations were proposed, as well as removal of one data point per analysis in the previous section.
```{r}
data$log_crime = log(data$crime)
data$log_police = log(data$police)
data$log_density = log(data$density)
data$pctmin = data$pctmin/100
data = data[data$X != 84,]

sort(colnames(data))
```
\pagebreak

## Bi-variate analysis

Looking at the correlation plot of the non-indicator variables, it looks like the following variables correlate with log of crime (last column in the plot) highly and positively:

1. log of density - This makes sense since the crime rate tends to increase in more populated areas.
2. tax - This makes sense since crime rate could increase in areas where there are more tax revenues collected.
3. wage variables - This somewhat makes sense since as wages go up, there may be higher likelihood for crime. 

It looks like the following variables correlate with log of crime (second-last column in the plot) highly and negatively:

1. probconv - This makes sense since if the probability of convictions goes down, then there are more (potential) criminals out on the streets.
2. probsen - This also makes sense since if the probability of sentencing goes down, then there are more (potential) criminals out on the streets.

```{r fig.height=6.5}
corrplot(cor(data[ , (names(data) %in% 
                        c("log_crime", "probarr", "probconv", "probsen", "tax", "log_police", "log_density",
                          "pctmin", "ymale", "wagecon", "wagetuc", "wagetrd", "wagefir", "wageser", "wagemfg", 
                          "wagefed", "wagesta", "wageloc", "mix"))]), method="circle", type="upper")
```

\pagebreak

#Section 3 - Model Specification and Assumptions

In our exploratory analysis, we identified key independent variables that were positively and negatively correlated with log of crime rate. To create our first and simplest model that contains variables of key interest, we included the subset of these variables that we hyothesized might be the most important determinants of crime.

It is "common knowledge" that areas with higher denisty have more crime, therefore we included log_density in our first model. In addition, we hypothesized that a high probability of arrest, sentencing and coviction would be deterrents of crime. While researching determinants of crime, we found that historically, one of the strongest determinants of crime has been the percent of young males in population, perhaps because they are the most likely perpetrators of crime. We therefore included this variable in our first model as well. In addition, we thought that higher taxes might increase the effectiveness of police and criminal justice capacities.

$$log(Crime Rate) = \beta_0 + {\beta_1}log(density)+ {\beta_2}conviction+ {\beta_3}sentencing+{\beta_4}arrests +{\beta_5}tax+{\beta_6}young male + u$$


$$\hat{log(Crime Rate)} = \hat{\beta_0} + \hat{\beta_1}log(density)+ \hat{\beta_2}conviction+ \hat{\beta_3}sentencing+\hat{\beta_4}arrests +\hat{\beta_5}tax +\hat{\beta_6}youngmale$$



```{r}
final_model1 = lm(log_crime ~ log_density + probsen + probconv + probarr + ymale + tax, data = data)
se.final_model1 = sqrt(diag(vcovHC(final_model1)))
```

In order to determine if our OLS coefficients will be unbiased estimates of the population parameters, we examined the six classic linear model assumptions for model 1.

### CLM.1 - Linear in Parameters

We chose our model specification so that the dependent variable is a linear function of the explanatory variables. Therefore, the CLM.1 assumption is met for our first model and all the other models we created.

### CLM.2 - Random Sampling

There were originally only 90 counties in the dataset. During our data cleaning, we removed 1 county because we judged that it contained an error in the input for the weekly wage of service employees variable. During our research into North Carolina, we discovered that there have been 100 counties in North Carolina since 1911. Therefore, our dataset does *not* contain every county in North Carolina. However, we didn't identify any indications of non-random sampling during our analysis. We therefore assume that the counties are a random sample of the 100 counties in North Carolina. Thus, the MLR.2 assumption is met for our first model and all the other models we created.

### CLM.3 - No Perfect Multicollinearity

From our EDA, it is apparent that none of our variables has constant values across the dataset. In addition, inspection of the correlation plot above indicates that there are no perfectly correlated variable pairs. In addition, analysis of the variance inflation factor for each variable does not provide evidence of multicollinearity.


```{r}
vif(final_model1)
```

Thus, the MLR.3 assumption is met for our first model and all the other models we created.

### CLM.4 - Zero Conditional Mean

By examining the residuals verus fitted values plot for our first model, we conclude that the assumption of zero conditional mean is met.  The red spline curve does not deviate much from zero.

```{r}
plot(final_model1, which = 1)
```

\pagebreak

### CLM.5 - Homoskedasticity

When we examined the residuals versus fitted values plot, it was apparent that the variance of errors to the right of the plot is smaller than the variance of errors in the middle and left of the plot. This suggested heteroskedasticity, so we examined the scale-location plot. The spline curve on the scale-location plot is curved rather than flat, indicating heteroskedasticity. 

Despite this clear violation of CLM.5, we are able to proceed with our OLS model by using heteroskedasticity-robust standard errors.

```{r}
plot(final_model1, which = 3)
```

\pagebreak

### CLM.6 - Normality of Residuals

Analysis of the qqplot of residuals for model 1 suggested that the residuals for model 1 were approximately normally distributed.  

```{r}
plot(final_model1, which = 2)
```

We further examined the distribution of the residuals in model 1 in a histogram. The residuals are somewhat normal, and in addition our sample size of 89 allows us to rely on asymptotics and the Central Limit Theorem. We can thus perform hypothesis testing of our OLS coefficients.


```{r}
hist(final_model1$residuals, breaks = 50, 
     main="Histogram of Model 1 Residuals", xlab="Residual")
```


Lastly, while there are several data points in our model with high leverage, their residuals are not large. Thus, none of the data points has a disconcertingly large Cook's distance and none cause undue influence on model fit.


```{r}
plot(final_model1, which=5)
```


###Conclusions from Examining CLM Assumptions and Interpretation of Model 1 Metrics

```{r}
coeftest(final_model1, vcov = vcovHC)
```


As seen in the regression table (below), the F statistic for the omnibus test of model 1 is 28.2 with a statistically significant p value of < 2.2e-16. We therefore reject the null hypothesis that the omnibus test indicates that model 2 is not different from zero.

The p values for the t tests for the coefficients on log_density, probconv, probsen, were all < 0.001 and ymale and tax were less than 0.05. For each coefficient, we therefore reject the null hypothesis that the coefficient is equal to zero. Because we will give a detailed summary of the coefficients for our favored model 2 (below), we won't discuss the practical significance of all the coefficients in model 1.

The adjusted R2 for model 1 is quite high for a social science study - 0.65. However, the Akaike Information Criterion (AIC) is 58.8 - fairly large compared to other models.


###Model 2: More Refined and More Robust than Model 1

After creating our first model, we carefully considered how robust the coefficients were for the variables we had included. We considered whether any of the variables we had not included in model 1 could be considered "omitted variables" - meaning, were they both correlated with log(crime) and with at least one other independent variable? 

To address these issues, we created more than twenty other models (not shown). The results of these analyses suggested that we should include percent minority and log(police per capita) in our second model, and (to our surprise) exluce percent young male because the coefficient for this variable was not robust across multiple models. The same was true for the tax variable.

$$log(Crime Rate) = \beta_0 + {\beta_1}log(density)+ {\beta_2}conviction+ {\beta_3}sentencing+{\beta_4}minority +{\beta_5}log(police) + u$$

$$\hat{log(Crime Rate)} = \hat{\beta_0} + \hat{\beta_1}log(density)+ \hat{\beta_2}conviction+ \hat{\beta_3}sentencing+\hat{\beta_4}minority +\hat{\beta_5}log(police)$$

```{r}
final_model2 = lm(log_crime ~ log_density + probsen + probconv  + pctmin+ log_police, data = data)
se.final_model2 = sqrt(diag(vcovHC(final_model2)))
```

###Conclusions from Examining CLM Assumptions and Interpretation of Model 2 Coefficients

Similar to our first model, we tested the CLM assumptions for model 2 and found that we only had to address CLM.4 (homoskedasticity) by using heteroskedasticity robust standard errors. 

Because our model only violates CLM.4 (homoskedasticity), we can conclude that our model coefficients will be unbiased estimators of the population parameters. In addition, we have little reason to suspect endogeneity and we feel comfortable interpreting particular coefficients in the model as causal.

```{r}
summary(final_model2)
coeftest(final_model2, vcov = vcovHC)

```


As seen in the regression table, the F statistic for the omnibus test of model 2 is 88.4 with a statistically significant p value of < 2.2e-16. We therefore reject the null hypothesis that the omnibus test indicates that model 2 is not different from zero.

The p values for the t tests for the coefficients on log_density, probsen, probconv, and pctmin are all less than 0.001. The p value for the t test for the coefficient of log_police is less than 0.01. Therefore, for every coefficient, we reject the null hypothesis that the coefficient is equal to zero.

The coefficient for log_density is 0.29. This suggests that for a one percentage point increase in density, there is a 0.29 percent increase in crime rate.

The coefficient for probsen is -0.56. This suggests that for a one unit increase in the probability of sentencing, there is a 56%  decrease in crime rate.

The coefficient for probconv is -1.70 This suggests that for a one unit increase in the probability of conviction, there is a 170%  decrease in crime rate.

The coefficient for pctmin is 1.33. This suggests that for a one unit increase in percent minority, there is a ??133%?? increase in crime rate.

The coefficient for log_police is 0.45. This suggests that for a one percentage point increase in police per capita, there is a 0.45 percent increase in crime rate.

*Practical significance:*

The adjusted R2 for this model (0.83) is better than for model 1 (0.65). In addition the AIC for model 2 is -7.7, indicating that the model predicts a very large percent of the variability in crime rate while being highly parsimonious compared to model 1 whose AIC is 58.8.


###Model 3: The "Kitchen Sink" Model 

To generate model 3, we added most of the other covariates from the dataset:

$$log(Crime Rate) = \beta_0 + {\beta_1}log(density)+ {\beta_2}conviction+ {\beta_3}sentencing+{\beta_4}arrests + {\beta_5}minority 
+{\beta_6}log(police) +{\beta_7}yougmale +{\beta_8}urban + {\beta_9}tax +{\beta_10}wage_con +{\beta_11}wage_tuc+{\beta_12}wage_trd+{\beta_13}wage_fir+{\beta_14}wage_mfg+{\beta_15}wage_fed+{\beta_16}wage_sta+{\beta_17}wage_loc+{\beta_18}wage_ser + u$$

$$\hat{log(Crime Rate)} = \hat{\beta_0} + \hat{\beta_1}log(density)+ \hat{\beta_2}conviction+  \hat{\beta_3}sentencing+ \hat{\beta_4}arrests +  \hat{\beta_5}minority + \hat{\beta_6}log(police) + \hat{\beta_7}yougmale + \hat{\beta_8}urban +  \hat{\beta_9}tax + \hat{\beta_10}wage_con + \hat{\beta_11}wage_tuc+ \hat{\beta_12}wage_trd+ \hat{\beta_13}wage_fir+ \hat{\beta_14}wage_mfg+ \hat{\beta_15}wage_fed+ \hat{\beta_16}wage_sta+ \hat{\beta_17}wage_loc+ \hat{\beta_18}wage_ser$$

```{r}
final_model3 = lm(log_crime ~ log_density + probsen + probconv + +probarr + ymale + tax + pctmin + 
                  log_police + urban + wagecon + wagetuc + wagetrd + wagefir + wagemfg + wagefed +
                  wagesta + wageloc + wageser, data = data)
se.final_model3 = sqrt(diag(vcovHC(final_model3)))
```

###Conclusions from Examining CLM Assumptions and Interpretation of Model 3


We found that Model 3, like our other models, only violated CLM.4 - we identified heteroskedasticity. To address this violation of CLM assumptions, we made use of heteroskedasticity-robust standard errors. Analysis of the variance inflation factor for the variables in model 3 did not indicate any serious multicollinearity. However, we note that the VIF for log(density) was greater than 4 in this model. We thus speculate that there is multicolinearity between the variables log(density), tax, and potentially log(police) in this model.

As seen in the regression table, the adjusted R2 for model 3 is 0.86. In addition, the AIC is -9.6. These metrics are indicators of a parsimonious model that can account for a huge amount of the variation in log(crime rate). However, we feel that this model is not as parsimonious as model 2 and includes variables that could unnecessarily increase variance of the coefficients resulting in potentially biased estimates. Thus, the primary purpose of this model is to support the idea that model 2 is a robust model. The coefficients of the variables from model 2 (log_density, probsen, probconv, pctmin & log_police) remain fairly constant compared to model 3 and these coefficients are still statistically significant in model 3 (see regression table below). 

Another reason that we do not favor model 3 is that the coefficient for percent young male is statistically significant in model 3. Of more than 20 models we tested, this coefficient is only statistically significant in model 1 and model 3 (as discussed above while justifying the variables to include in model 2). This supports both the idea that the contribution of percent young male to log(crime rate) is not robust and also the idea that model 3 is not a robust model. In addition, model 3 contains many variables whose coefficients are not statistically significant from zero (see regression table). Thus we favor model 2, because it is the most robust model we have identified and it also accounts for a large amount of variation in log(crime rate).



```{r}
stargazer(final_model1, final_model2, final_model3,
        se=list(se.final_model1, se.final_model2, se.final_model3),
        star.cutoffs=c(0.05, 0.01, 0.001), title = "Regression Table for 3 Models", 
        column.labels = c("Model1 - OK", "Model2 - Great", "Model3 - Kitchen Sink"),
        type="text")
```


### Causality

The proposed model (model 2) indicates that harsher sentencing laws are the primary deterrent to crime, whereas police per capita does not improve the crime rate. While percentage minority does appear to have an impact on crime, the coefficient is small and the below analysis of omitted variables indicates that even this coefficient is likely overstated.

A number of metrics that are likely present in the true population model for predicting crime are not available for this analysis, and this impacts the overall predictability of our model beyond the current dataset. Police per person is a key variable exhibiting the wrong sign. In theory, police should reduce crime rather than increase it (as is currently predicted in our model).  The below analysis of omitted variables underscores the complexity of assessing the impact of a police force among other factors on crime.

_1. Police Profiling_ may increase the likelihood of minority groups and young males being arrested for crimes they do not commit. The coefficients of the pctmin and ymale variables are likely overstated without accounting for profiling.

_2. Recidivism_ (the likelihood for people to become repeat offenders) is likely a very strong predictor of crime in specific regions. It would make sense that areas with higher crime overall, have higher incidences of recidivism, and therefore more police per capita. If recidivism and police per capita are positively correlated, then the police variable is absorbing some of the error term associated with the omitted variable, recidivism. Including this metric would likely help change the sign of the police variable; this would lead to a more intuitive model since police should be associated with reducing (not increasing) crime.

_3. Police Motivation_ may also have an impact on probability of arrest. For example, if crime is very prevalent in one area while convictions or sentences in that same area are low, police may feel that their efforts are not having an impact. This could have either a positive or negative impact on the probarr variable, though a negative impact is more likely as police are less inspired to pursue arrests. In this case, fewer arrests would be reported for each crime and this factor would be unaccounted for in the model.

_4. Security_: Home security services such as ADT or Ring may reduce the prevalence of crime. Greater security around businesses (e.g. density of security guards) would have the same effect. Omitting these variables likely over-attributes crime reduction to stricter laws and more police than is accurate in the true population. Therefore probsen, probconv and police per capita are likely overstated in terms of reducing crime. (Note that this would make the police metric even more positive than it currently is). Additionally, with a security metric, the mix variable related to property crimes may have more meaning in the data.

_5. The unemployment rate and education levels_ could help explain incidences of crime, and together these metrics may be better measures of socioeconomic status than total wages or tax per person. At the very least, one of the four metrics (or interactions between them) may allow socioeconomic status to have a significant role in the model.

_6. Proximity to other high crime regions_ could be a notable factor to examine as well.  For example, a region with low density and high crime may be contiguous to another region with high density/high crime, helping to further explain the relationship between crime and density.

####Conclusions


